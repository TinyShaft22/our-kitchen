---
phase: 28-cooking-mode
plan: 02
type: execute
wave: 2
depends_on: ["28-01"]
files_modified:
  - our-kitchen-alexa/lambda/handlers/CookingHandlers.js
  - our-kitchen-alexa/lambda/index.js
autonomous: true

must_haves:
  truths:
    - "User can say 'start cooking' to enter cooking mode from recipe view"
    - "User can say 'next step' to advance through recipe steps"
    - "User can say 'previous step' to go back to prior step"
    - "User can say 'repeat' to hear current step again"
    - "Each step is read aloud when navigated to"
    - "Last step says 'You're done! Enjoy your meal.' and offers to return"
    - "Voice-only devices work (same voice output, no APL errors)"
  artifacts:
    - path: "our-kitchen-alexa/lambda/handlers/CookingHandlers.js"
      provides: "Cooking mode intent handlers"
      exports: ["StartCookingIntentHandler", "NextStepIntentHandler", "PreviousStepIntentHandler", "RepeatStepIntentHandler"]
    - path: "our-kitchen-alexa/lambda/index.js"
      provides: "Updated handler registration"
      contains: "CookingHandlers"
  key_links:
    - from: "our-kitchen-alexa/lambda/handlers/CookingHandlers.js"
      to: "apl/cooking-step.json"
      via: "RenderDocument directive in cooking handlers"
      pattern: "addDirective.*RenderDocument.*cooking"
    - from: "our-kitchen-alexa/lambda/index.js"
      to: "handlers/CookingHandlers.js"
      via: "require and addRequestHandlers"
      pattern: "require.*CookingHandlers"
---

<objective>
Implement voice-controlled cooking mode handlers for step-by-step recipe navigation.

Purpose: Enable hands-free cooking with voice commands ("next step", "previous step", "repeat") while auto-reading each step aloud. Works on all Alexa devices (voice-only and Echo Show).

Output:
- CookingHandlers.js with StartCooking, Next, Previous, Repeat handlers
- Updated index.js with handler registration
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/28-cooking-mode/28-CONTEXT.md
@.planning/phases/28-cooking-mode/28-01-SUMMARY.md (after Plan 01 completes)
@our-kitchen-alexa/lambda/handlers/MealHandlers.js
@our-kitchen-alexa/lambda/index.js
</context>

<notes>
**Auto-offer cooking mode (CONTEXT.md requirement):**
This requirement is ALREADY SATISFIED by Phase 27's GetRecipeIntentHandler (MealHandlers.js line ~155).
After viewing a recipe, Alexa says: "Would you like me to read the ingredients, read the instructions, or start cooking mode?"
This IS the auto-offer - no additional work needed in this phase.

**Deferred to Phase 30:**
- In-skill timers ("Set timer for 10 minutes")
</notes>

<tasks>

<task type="auto">
  <name>Task 1: Create CookingHandlers.js with all cooking mode handlers</name>
  <files>
    our-kitchen-alexa/lambda/handlers/CookingHandlers.js
  </files>
  <action>
Create CookingHandlers.js with handlers for cooking mode navigation.

**File: our-kitchen-alexa/lambda/handlers/CookingHandlers.js**

**Imports:**
```javascript
const Alexa = require('ask-sdk-core');
const { getRecipe } = require('../api/firebaseClient');
const { isLinked, getHouseholdCode } = require('../util/sessionHelper');
const { createPinPromptResponse } = require('./HouseholdHandlers');
const { parseInstructionsToSteps } = require('../util/stepParser');
const cookingStepDocument = require('../apl/cooking-step.json');
const { buildCookingStepDataSource } = require('../apl/cooking-step-data');
```

**1. StartCookingIntentHandler**

Handles: "Let's cook {MealName}" / "Start cooking" (from recipe view)

Logic:
- If MealName slot provided: fetch recipe by name match (same as GetRecipeIntent)
- If no MealName but currentRecipe in session: use that recipe
- If neither: prompt "Which recipe would you like to cook?"
- Parse instructions into steps using parseInstructionsToSteps
- Store in session: cookingMode: true, cookingSteps: steps array, cookingStep: 0, cookingRecipe: recipe
- Speak Step 0 (ingredients): "Let's make {name}. First, gather your ingredients: {list}. Say 'next step' when ready."
- Add APL directive if device supports it

```javascript
const StartCookingIntentHandler = {
  canHandle(handlerInput) {
    return Alexa.getRequestType(handlerInput.requestEnvelope) === 'IntentRequest'
      && Alexa.getIntentName(handlerInput.requestEnvelope) === 'StartCookingIntent';
  },
  async handle(handlerInput) {
    if (!isLinked(handlerInput)) {
      return createPinPromptResponse(handlerInput, 'StartCooking');
    }

    const sessionAttributes = handlerInput.attributesManager.getSessionAttributes();
    const slots = handlerInput.requestEnvelope.request.intent.slots;
    const mealNameSlot = slots?.MealName;

    let recipe = null;

    // Option 1: Meal name provided in slot
    if (mealNameSlot?.value) {
      const householdCode = getHouseholdCode(handlerInput);
      const lastMealList = sessionAttributes.lastMealList || [];
      const matchedMeal = lastMealList.find(m =>
        m.name.toLowerCase().includes(mealNameSlot.value.toLowerCase()) ||
        mealNameSlot.value.toLowerCase().includes(m.name.toLowerCase())
      );

      if (matchedMeal) {
        recipe = await getRecipe(householdCode, matchedMeal.id);
      }
    }

    // Option 2: Use current recipe from session (viewing a recipe)
    if (!recipe && sessionAttributes.currentRecipe) {
      recipe = sessionAttributes.currentRecipe;
    }

    // No recipe found
    if (!recipe || !recipe.name) {
      return handlerInput.responseBuilder
        .speak("Which recipe would you like to cook? Try asking to see the recipe first.")
        .reprompt("What recipe do you want to cook?")
        .getResponse();
    }

    // Parse steps
    const steps = parseInstructionsToSteps(recipe.instructions || '', recipe.ingredients || []);

    // Store cooking mode state
    sessionAttributes.cookingMode = true;
    sessionAttributes.cookingSteps = steps;
    sessionAttributes.cookingStep = 0;
    sessionAttributes.cookingRecipe = recipe;
    handlerInput.attributesManager.setSessionAttributes(sessionAttributes);

    // Build voice output for Step 0 (ingredients)
    const ingredientList = (recipe.ingredients || []).map(i => i.name).join(', ');
    const speakOutput = `Let's make ${recipe.name}. First, gather your ingredients: ${ingredientList}. Say 'next step' when you're ready to begin.`;

    const responseBuilder = handlerInput.responseBuilder
      .speak(speakOutput)
      .reprompt("Say 'next step' to continue, or 'stop' to exit.");

    // Add APL if supported
    if (Alexa.getSupportedInterfaces(handlerInput.requestEnvelope)['Alexa.Presentation.APL']) {
      responseBuilder.addDirective({
        type: 'Alexa.Presentation.APL.RenderDocument',
        token: 'cookingStepToken',
        document: cookingStepDocument,
        datasources: buildCookingStepDataSource(recipe, 0)
      });
    }

    return responseBuilder.getResponse();
  }
};
```

**2. NextStepIntentHandler**

Handles: AMAZON.NextIntent when in cooking mode

Logic:
- Check cookingMode in session - if not in cooking mode, delegate to fallback
- Increment cookingStep (capped at max)
- If at last step: "You're done! Enjoy your meal. Say 'go back' to see the recipe again."
- Otherwise: Read step content, prompt for next
- Update APL pager

```javascript
const NextStepIntentHandler = {
  canHandle(handlerInput) {
    const sessionAttributes = handlerInput.attributesManager.getSessionAttributes();
    return Alexa.getRequestType(handlerInput.requestEnvelope) === 'IntentRequest'
      && Alexa.getIntentName(handlerInput.requestEnvelope) === 'AMAZON.NextIntent'
      && sessionAttributes.cookingMode === true;
  },
  handle(handlerInput) {
    const sessionAttributes = handlerInput.attributesManager.getSessionAttributes();
    const steps = sessionAttributes.cookingSteps || [];
    let currentStep = sessionAttributes.cookingStep || 0;
    const recipe = sessionAttributes.cookingRecipe;

    // Advance step
    currentStep = Math.min(currentStep + 1, steps.length - 1);
    sessionAttributes.cookingStep = currentStep;
    handlerInput.attributesManager.setSessionAttributes(sessionAttributes);

    const step = steps[currentStep];
    const isLastStep = currentStep === steps.length - 1;

    let speakOutput;
    if (isLastStep && !step.isIngredients) {
      speakOutput = `${step.content} <break time="500ms"/> You're done! Enjoy your meal. Say 'go back' to see the recipe, or 'stop' to exit.`;
    } else {
      speakOutput = `${step.title}. ${step.content} <break time="300ms"/> Say 'next step' to continue.`;
    }

    const responseBuilder = handlerInput.responseBuilder
      .speak(speakOutput)
      .reprompt("Say 'next step', 'previous step', or 'repeat'.");

    // Update APL pager
    if (Alexa.getSupportedInterfaces(handlerInput.requestEnvelope)['Alexa.Presentation.APL']) {
      responseBuilder.addDirective({
        type: 'Alexa.Presentation.APL.ExecuteCommands',
        token: 'cookingStepToken',
        commands: [{
          type: 'SetPage',
          componentId: 'stepPager',
          position: 'absolute',
          value: currentStep
        }]
      });
    }

    return responseBuilder.getResponse();
  }
};
```

**3. PreviousStepIntentHandler**

Handles: AMAZON.PreviousIntent when in cooking mode

Logic:
- Check cookingMode - if not, delegate to fallback
- Decrement cookingStep (min 0)
- Read step content
- Update APL pager

```javascript
const PreviousStepIntentHandler = {
  canHandle(handlerInput) {
    const sessionAttributes = handlerInput.attributesManager.getSessionAttributes();
    return Alexa.getRequestType(handlerInput.requestEnvelope) === 'IntentRequest'
      && Alexa.getIntentName(handlerInput.requestEnvelope) === 'AMAZON.PreviousIntent'
      && sessionAttributes.cookingMode === true;
  },
  handle(handlerInput) {
    const sessionAttributes = handlerInput.attributesManager.getSessionAttributes();
    const steps = sessionAttributes.cookingSteps || [];
    let currentStep = sessionAttributes.cookingStep || 0;
    const recipe = sessionAttributes.cookingRecipe;

    // Go back
    currentStep = Math.max(currentStep - 1, 0);
    sessionAttributes.cookingStep = currentStep;
    handlerInput.attributesManager.setSessionAttributes(sessionAttributes);

    const step = steps[currentStep];
    const speakOutput = `${step.title}. ${step.content} <break time="300ms"/> Say 'next step' to continue.`;

    const responseBuilder = handlerInput.responseBuilder
      .speak(speakOutput)
      .reprompt("Say 'next step', 'previous step', or 'repeat'.");

    // Update APL pager
    if (Alexa.getSupportedInterfaces(handlerInput.requestEnvelope)['Alexa.Presentation.APL']) {
      responseBuilder.addDirective({
        type: 'Alexa.Presentation.APL.ExecuteCommands',
        token: 'cookingStepToken',
        commands: [{
          type: 'SetPage',
          componentId: 'stepPager',
          position: 'absolute',
          value: currentStep
        }]
      });
    }

    return responseBuilder.getResponse();
  }
};
```

**4. RepeatStepIntentHandler**

Handles: AMAZON.RepeatIntent when in cooking mode

Logic:
- Check cookingMode - if not, delegate to fallback
- Re-read current step content

```javascript
const RepeatStepIntentHandler = {
  canHandle(handlerInput) {
    const sessionAttributes = handlerInput.attributesManager.getSessionAttributes();
    return Alexa.getRequestType(handlerInput.requestEnvelope) === 'IntentRequest'
      && Alexa.getIntentName(handlerInput.requestEnvelope) === 'AMAZON.RepeatIntent'
      && sessionAttributes.cookingMode === true;
  },
  handle(handlerInput) {
    const sessionAttributes = handlerInput.attributesManager.getSessionAttributes();
    const steps = sessionAttributes.cookingSteps || [];
    const currentStep = sessionAttributes.cookingStep || 0;

    const step = steps[currentStep];
    if (!step) {
      return handlerInput.responseBuilder
        .speak("I'm not sure what to repeat. Try saying 'next step'.")
        .reprompt("What would you like to do?")
        .getResponse();
    }

    const speakOutput = `${step.title}. ${step.content}`;

    return handlerInput.responseBuilder
      .speak(speakOutput)
      .reprompt("Say 'next step', 'previous step', or 'repeat'.")
      .getResponse();
  }
};
```

**Export all handlers:**
```javascript
module.exports = {
  StartCookingIntentHandler,
  NextStepIntentHandler,
  PreviousStepIntentHandler,
  RepeatStepIntentHandler
};
```
  </action>
  <verify>
Verify CookingHandlers loads:
```bash
cd our-kitchen-alexa/lambda && node -e "
const h = require('./handlers/CookingHandlers');
console.log('StartCooking:', typeof h.StartCookingIntentHandler);
console.log('Next:', typeof h.NextStepIntentHandler);
console.log('Previous:', typeof h.PreviousStepIntentHandler);
console.log('Repeat:', typeof h.RepeatStepIntentHandler);
"
```
  </verify>
  <done>All cooking mode handlers created with voice navigation and APL updates</done>
</task>

<task type="auto">
  <name>Task 2: Update index.js to register cooking handlers</name>
  <files>
    our-kitchen-alexa/lambda/index.js
  </files>
  <action>
Update index.js to import and register the new cooking handlers.

**Changes to our-kitchen-alexa/lambda/index.js:**

1. Add import near other handler imports:
```javascript
const {
  StartCookingIntentHandler,
  NextStepIntentHandler,
  PreviousStepIntentHandler,
  RepeatStepIntentHandler
} = require('./handlers/CookingHandlers');
```

2. Add handlers to addRequestHandlers() call.

**IMPORTANT handler order:** The context-aware handlers (NextStepIntentHandler, PreviousStepIntentHandler, RepeatStepIntentHandler) must be registered BEFORE any generic handlers for the same intents. Since these check for `cookingMode === true` in canHandle, they'll only match when in cooking mode - others fall through to generic handling.

Insert the cooking handlers BEFORE the existing HelloWorldIntentHandler:
```javascript
.addRequestHandlers(
    LaunchRequestHandler,
    LinkHouseholdIntentHandler,
    BrowseMealsIntentHandler,
    GetRecipeIntentHandler,
    BrowseCategoryIntentHandler,
    StartCookingIntentHandler,        // NEW - must be before groceries
    NextStepIntentHandler,            // NEW - context-aware
    PreviousStepIntentHandler,        // NEW - context-aware
    RepeatStepIntentHandler,          // NEW - context-aware
    ReadGroceryListIntentHandler,
    AddGroceryIntentHandler,
    UndoGroceryIntentHandler,
    RemoveGroceryIntentHandler,
    CheckOffGroceryIntentHandler,
    MealSelectedEventHandler,
    HelloWorldIntentHandler,
    HelpIntentHandler,
    CancelAndStopIntentHandler,
    FallbackIntentHandler,
    SessionEndedRequestHandler
)
```
  </action>
  <verify>
Verify full Lambda loads:
```bash
cd our-kitchen-alexa/lambda && node -e "
require('./index.js');
console.log('Lambda loaded successfully');
"
```
  </verify>
  <done>Cooking handlers registered in index.js with correct order</done>
</task>

</tasks>

<verification>
All cooking mode handlers integrated:

1. CookingHandlers.js exists and exports all handlers:
```bash
cd our-kitchen-alexa/lambda && node -e "
const h = require('./handlers/CookingHandlers');
console.log('Exports:', Object.keys(h).join(', '));
"
```

2. index.js imports and registers handlers:
```bash
grep "CookingHandlers" our-kitchen-alexa/lambda/index.js
grep "StartCookingIntentHandler" our-kitchen-alexa/lambda/index.js
```

3. Full Lambda validation:
```bash
cd our-kitchen-alexa/lambda && node -e "require('./index.js')"
```
</verification>

<success_criteria>
- StartCookingIntentHandler enters cooking mode with Step 0 (ingredients)
- NextStepIntentHandler advances through steps with auto-read
- PreviousStepIntentHandler goes back with auto-read
- RepeatStepIntentHandler re-reads current step
- Last step shows completion message
- APL pager updates on voice navigation
- Voice-only devices work without errors
- Lambda loads successfully
</success_criteria>

<output>
After completion, create `.planning/phases/28-cooking-mode/28-02-SUMMARY.md`
</output>
