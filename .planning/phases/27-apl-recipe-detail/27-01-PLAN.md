---
phase: 27-apl-recipe-detail
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - our-kitchen-alexa/lambda/apl/recipe-detail.json
  - our-kitchen-alexa/lambda/apl/recipe-detail-data.js
  - our-kitchen-alexa/lambda/handlers/MealHandlers.js
  - our-kitchen-alexa/lambda/handlers/AplEventHandlers.js
autonomous: true

must_haves:
  truths:
    - "Echo Show displays recipe with ingredients and instructions when user requests a recipe"
    - "Voice confirms 'Here's the recipe for {meal}' then offers follow-up actions"
    - "Touch selection from Phase 26 meal list shows recipe detail immediately"
    - "Voice-only devices still work (same voice output, no APL)"
    - "Follow-up prompts offer 'read ingredients, read instructions, or start cooking mode'"
  artifacts:
    - path: "our-kitchen-alexa/lambda/apl/recipe-detail.json"
      provides: "APL document for recipe detail view"
      contains: "AlexaDetail"
    - path: "our-kitchen-alexa/lambda/apl/recipe-detail-data.js"
      provides: "DataSource builder for recipe content"
      exports: ["buildRecipeDetailDataSource"]
  key_links:
    - from: "our-kitchen-alexa/lambda/handlers/MealHandlers.js"
      to: "apl/recipe-detail.json"
      via: "RenderDocument directive in GetRecipeIntentHandler"
      pattern: "addDirective.*RenderDocument.*recipe"
    - from: "our-kitchen-alexa/lambda/handlers/AplEventHandlers.js"
      to: "apl/recipe-detail.json"
      via: "RenderDocument directive in MealSelectedEventHandler"
      pattern: "addDirective.*RenderDocument.*recipe"
---

<objective>
Display recipe details (ingredients and instructions) on Echo Show when user requests a recipe by voice or touch.

Purpose: Enable hands-free recipe viewing with visual display showing ingredients list and instructions text, while maintaining the voice-first conversational flow ("Here's the recipe... Would you like me to read the ingredients?").

Output:
- APL document for recipe detail view (not a list - single recipe display)
- DataSource builder for recipe content
- Updated GetRecipeIntentHandler with APL directive
- Updated MealSelectedEventHandler to fetch and display recipe immediately
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/27-apl-recipe-detail/27-CONTEXT.md
@.planning/phases/26-apl-meal-list/26-01-SUMMARY.md
@our-kitchen-alexa/lambda/apl/meal-list.json
@our-kitchen-alexa/lambda/apl/meal-list-data.js
@our-kitchen-alexa/lambda/handlers/MealHandlers.js
@our-kitchen-alexa/lambda/handlers/AplEventHandlers.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create APL document and datasource builder for recipe detail</name>
  <files>
    our-kitchen-alexa/lambda/apl/recipe-detail.json
    our-kitchen-alexa/lambda/apl/recipe-detail-data.js
  </files>
  <action>
Create two files in the apl/ directory:

**recipe-detail.json** - APL document using AlexaDetail (responsive detail layout):
- APL version 2024.3
- Import alexa-layouts 1.7.0 and alexa-viewport-profiles 1.6.0
- Same custom resources as meal-list.json (dark theme #1A1A1A, terracotta accent #C4704B)
- mainTemplate with parameter "recipeDetailData"
- Use AlexaDetail component (or Container with responsive layout if AlexaDetail doesn't fit):
  - id: "recipeDetailView"
  - headerTitle bound to "${recipeDetailData.mealName}"
  - headerBackButton: true (allows user to go back)
  - backgroundImageSource: "${recipeDetailData.imageUrl}" (optional, can be empty)
  - Body content:
    - Left column or top section: Ingredients list
      - Header "Ingredients"
      - Text or list of ingredient names (simple bullet format or numbered)
    - Right column or bottom section: Instructions
      - Header "Instructions"
      - ScrollView with Text for instructions (markdown already converted to plain text)
  - Use responsive layout that adapts to screen size (larger screens show side-by-side, smaller show stacked)
  - theme: "dark"

Note: AlexaDetail is ideal for detail screens. If AlexaDetail doesn't support the two-column layout needed, use a Container with when conditions for viewport profiles. Keep it simple - ingredients on top/left, instructions below/right.

**recipe-detail-data.js** - DataSource builder:
- Export buildRecipeDetailDataSource(recipe)
- Input: recipe object with { id, name, imageUrl, ingredients: [{name}], instructions: string }
- Returns object with recipeDetailData key containing:
  - type: 'object'
  - objectId: 'recipeDetailDS'
  - mealName: recipe name
  - imageUrl: recipe.imageUrl or '' (for background)
  - ingredients: array of ingredient names (mapped from recipe.ingredients)
  - instructions: recipe.instructions or 'No instructions available'
  </action>
  <verify>
Files exist:
- ls our-kitchen-alexa/lambda/apl/recipe-detail.json
- ls our-kitchen-alexa/lambda/apl/recipe-detail-data.js

Syntax check:
- node -e "JSON.parse(require('fs').readFileSync('./our-kitchen-alexa/lambda/apl/recipe-detail.json'))"
- node -e "require('./our-kitchen-alexa/lambda/apl/recipe-detail-data.js')"
  </verify>
  <done>APL document and datasource builder exist with correct structure for recipe detail view</done>
</task>

<task type="auto">
  <name>Task 2: Update GetRecipeIntentHandler with APL directive</name>
  <files>
    our-kitchen-alexa/lambda/handlers/MealHandlers.js
  </files>
  <action>
Update MealHandlers.js to add APL visual for GetRecipeIntentHandler:

1. Add imports at top (near existing APL imports):
   const recipeDetailDocument = require('../apl/recipe-detail.json');
   const { buildRecipeDetailDataSource } = require('../apl/recipe-detail-data');

2. In GetRecipeIntentHandler.handle(), update the voice flow per CONTEXT.md:
   - Change speakOutput to: "Here's the recipe for {result.name}. Would you like me to read the ingredients, read the instructions, or start cooking mode?"
   - Keep the reprompt asking about follow-up actions

3. Add APL directive after building response (same pattern as BrowseMealsIntentHandler):
   - Check if device supports APL: Alexa.getSupportedInterfaces(handlerInput.requestEnvelope)['Alexa.Presentation.APL']
   - If supported, add directive:
     ```
     responseBuilder.addDirective({
       type: 'Alexa.Presentation.APL.RenderDocument',
       token: 'recipeDetailToken',
       document: recipeDetailDocument,
       datasources: buildRecipeDetailDataSource(result)
     });
     ```

4. Keep session storage for cooking mode (currentRecipe, currentRecipeStep)

The voice output changes from reading ingredients immediately to offering the "hub" flow where the screen shows everything and user chooses what to hear.
  </action>
  <verify>
Code patterns present:
- grep "recipeDetailDocument" our-kitchen-alexa/lambda/handlers/MealHandlers.js
- grep "buildRecipeDetailDataSource" our-kitchen-alexa/lambda/handlers/MealHandlers.js
- grep "'Alexa.Presentation.APL.RenderDocument'" our-kitchen-alexa/lambda/handlers/MealHandlers.js | wc -l
  (should be 2 - one for meal list, one for recipe detail)

Lambda loads:
- cd our-kitchen-alexa/lambda && node -e "require('./handlers/MealHandlers.js')"
  </verify>
  <done>GetRecipeIntentHandler displays APL and offers follow-up actions</done>
</task>

<task type="auto">
  <name>Task 3: Update MealSelectedEventHandler to show recipe detail</name>
  <files>
    our-kitchen-alexa/lambda/handlers/AplEventHandlers.js
  </files>
  <action>
Update AplEventHandlers.js to fetch and display recipe when user taps a meal:

1. Add imports at top:
   const { getRecipe } = require('../api/firebaseClient');
   const { isLinked, getHouseholdCode } = require('../util/sessionHelper');
   const recipeDetailDocument = require('../apl/recipe-detail.json');
   const { buildRecipeDetailDataSource } = require('../apl/recipe-detail-data');

2. Update MealSelectedEventHandler.handle():
   - Get householdCode using getHouseholdCode(handlerInput)
   - Get mealId from args[2] (already there)
   - Fetch recipe: const result = await getRecipe(householdCode, mealId);
   - Handle error case: if (!result || !result.name), speak error and return
   - Store recipe in session (for follow-up actions):
     sessionAttributes.currentRecipe = result;
     sessionAttributes.selectedMealId = mealId;
     sessionAttributes.navigationSource = 'mealList';
   - Build voice output per CONTEXT.md: "Here's the recipe for {result.name}. Would you like me to read the ingredients, read the instructions, or start cooking mode?"
   - Add APL directive (device already supports APL since they tapped):
     ```
     handlerInput.responseBuilder.addDirective({
       type: 'Alexa.Presentation.APL.RenderDocument',
       token: 'recipeDetailToken',
       document: recipeDetailDocument,
       datasources: buildRecipeDetailDataSource(result)
     });
     ```
   - Return response with speak and reprompt

This makes touch selection navigate directly to recipe detail (same as voice "show me the recipe for X").
  </action>
  <verify>
Code patterns present:
- grep "getRecipe" our-kitchen-alexa/lambda/handlers/AplEventHandlers.js
- grep "buildRecipeDetailDataSource" our-kitchen-alexa/lambda/handlers/AplEventHandlers.js
- grep "recipeDetailToken" our-kitchen-alexa/lambda/handlers/AplEventHandlers.js

Lambda loads:
- cd our-kitchen-alexa/lambda && node -e "require('./handlers/AplEventHandlers.js')"

Full Lambda validation:
- cd our-kitchen-alexa/lambda && node -e "require('./index.js')"
  </verify>
  <done>Touch selection from meal list shows recipe detail immediately</done>
</task>

</tasks>

<verification>
All recipe detail APL components integrated:

1. APL document exists for recipe detail:
   - cat our-kitchen-alexa/lambda/apl/recipe-detail.json | head -20

2. DataSource builder exports correctly:
   - node -e "const {buildRecipeDetailDataSource} = require('./our-kitchen-alexa/lambda/apl/recipe-detail-data.js'); console.log(typeof buildRecipeDetailDataSource)"

3. GetRecipeIntentHandler has APL directive:
   - grep "recipeDetailToken" our-kitchen-alexa/lambda/handlers/MealHandlers.js

4. MealSelectedEventHandler fetches recipe and shows APL:
   - grep "recipeDetailToken" our-kitchen-alexa/lambda/handlers/AplEventHandlers.js

5. Voice output follows CONTEXT.md flow:
   - grep "read the ingredients" our-kitchen-alexa/lambda/handlers/MealHandlers.js
   - grep "read the ingredients" our-kitchen-alexa/lambda/handlers/AplEventHandlers.js

6. Full Lambda validates:
   - cd our-kitchen-alexa/lambda && node -e "require('./index.js')"
</verification>

<success_criteria>
- APL document displays recipe with ingredients list and instructions text
- Both voice ("Show me the recipe for X") and touch (tap on meal list) show recipe detail
- Voice output follows "Here's the recipe... Would you like me to..." pattern
- Voice-only devices unaffected (same functionality, no APL errors)
- Dark theme with terracotta accent consistent with Phase 26
- Lambda loads without errors
</success_criteria>

<output>
After completion, create `.planning/phases/27-apl-recipe-detail/27-01-SUMMARY.md`
</output>
